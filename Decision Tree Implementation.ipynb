{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Node Class for the Decision Tree......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are just creating the node for the tree.....where\n",
    " #feature-> the feature on which we splitted the tree...\n",
    " #value->the value of the continuous feature at wich we alitted\n",
    " #output->the output for the current node if we donot split further\n",
    " #gain_ratio-> gain_ratio for that node....\n",
    " #y will store the y_train of that node....which will later be used to calculate the entropy for that node.....\n",
    "class Node:\n",
    "    def __init__(self,feature=None,value=None,output=None,gain_ratio=None,level=None,y=None):\n",
    "        self.feature=feature;\n",
    "        self.value=value;\n",
    "        self.output=output;\n",
    "        self.gain_ratio=gain_ratio;\n",
    "        self.level=level;\n",
    "        self.y=y;\n",
    "        self.right=None;\n",
    "        self.left=None;\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating All the Functions which are requied for making the Decision Tree...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split_info(d1,d2):\n",
    "    d=d1+d2;\n",
    "    num1 = d1/d\n",
    "    num2 = d2/d\n",
    "    return (-1)*((num1*math.log2(num1)/math.log2(2))+(num2*math.log2(num2)/math.log2(2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entropy(y):\n",
    "    dic = Counter(y)\n",
    "    entropy = 0\n",
    "    keys = dic.keys()\n",
    "    total = len(y)\n",
    "    for i in keys:\n",
    "        key_value = dic[i]\n",
    "        p = (key_value/total)\n",
    "        entropy+=-1*(p*math.log2(p));\n",
    "    return entropy;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gain_ratio(x,y,i):\n",
    "    list1=x[:,i].copy();\n",
    "    list1.sort();\n",
    "    value=0;\n",
    "    parent_entropy=find_entropy(y);\n",
    "    gain_ratio=-1;\n",
    "    for j in range(0,len(list1)-1):\n",
    "        avg=(list1[j]+list1[j+1])/2;\n",
    "        #suppose if list1 is[2,2,3,4,4,5,6,] so we will get avg for 1st time as (2+2)/2=2 and we need to avoid that\n",
    "        #so used this if condition...\n",
    "        if(avg==list1[j]):\n",
    "            continue;\n",
    "            \n",
    "        less_than_avg_y=y[x[:,i]<avg];\n",
    "        more_than_avg_y=y[x[:,i]>=avg];\n",
    "        d1=len(less_than_avg_y);\n",
    "        d2=len(more_than_avg_y);\n",
    "        \n",
    "        split_info=find_split_info(d1,d2);\n",
    "        \n",
    "        \n",
    "        e1=find_entropy(less_than_avg_y);\n",
    "        e2=find_entropy(more_than_avg_y);\n",
    "        \n",
    "        \n",
    "        info_req=e1*(d1/(d1+d2))+e2*(d2/(d1+d2));\n",
    "        info_gain=parent_entropy-info_req;\n",
    "        temp_gain_ratio=info_gain/split_info;\n",
    "        \n",
    "        if(temp_gain_ratio>=gain_ratio):\n",
    "            gain_ratio=temp_gain_ratio;\n",
    "            value=avg;\n",
    "     # it returns the gain_ratio at which we are splitting and also the value of the feature at which we are splitting.\n",
    "    # to the find_feature_to_split() function which has called this function..........\n",
    "    return [gain_ratio,value];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are many features available for the splitting so it will find the feature which provide the best gain ratio and returns the feature_no,value pt of feature at which we split(because the datapoints are continuous),and the gain ratio at which we split Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_feature_to_split(x,y):\n",
    "    # feature_no has the value that we are taking th 0th ,1st or which feature\n",
    "    feature_no=0;\n",
    "    value=0;\n",
    "    gain_ratio=-1;\n",
    "    for i in range(0,x.shape[1]):\n",
    "        list1=find_gain_ratio(x,y,i);\n",
    "        temp_gain_ratio=list1[0];\n",
    "        temp_value=list1[1];\n",
    "        if(temp_gain_ratio>gain_ratio):\n",
    "            gain_ratio=temp_gain_ratio;\n",
    "            feature_no=i;\n",
    "            value=temp_value;\n",
    "     #It returns the feature_no,the value of feature for split and the gain_ratio for split.       \n",
    "    return [feature_no,value,gain_ratio];\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell is used for the construction of the tree....if u need to make a tree then call Construct_tree function with the input and output of train_dataset....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tree(x_train,y_train,level=0):\n",
    "    # base case i.e. pure node\n",
    "    if(len(set(y_train))==1):\n",
    "        return Node(output=y_train[0],gain_ratio=0.0,level=level,y=y_train);\n",
    "    \n",
    "    list1=find_feature_to_split(x_train,y_train);\n",
    "\n",
    "    \n",
    "    # this list contains the feature_name,,,value of feature at which we are splitting,,,and the gain_ratio for the split\n",
    "    node =Node(list1[0],list1[1],Counter(y_train).most_common()[0][0],list1[2],level,y_train);\n",
    "    \n",
    "    node.left=construct_tree(x_train[x_train[:,list1[0]]>=list1[1]],y_train[x_train[:,list1[0]]>=list1[1]],level+1);\n",
    "    node.right=construct_tree(x_train[x_train[:,list1[0]]<list1[1]],y_train[x_train[:,list1[0]]<list1[1]],level+1);\n",
    "    return node;\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Of Tree Is over............."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's Time to Print the tree\n",
    "## And we are Using the Print Funtion to print the tree and this function takes the root of the tree and the feature_names of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(root,feature):\n",
    "    print(\"Level\",root.level);\n",
    "    temp=Counter(root.y).most_common();\n",
    "    if(len(temp)==1):\n",
    "        print(\"Count of\",temp[0][0],\"=\",temp[0][1]);\n",
    "        print(\"Current Entropy is =\",find_entropy(root.y));\n",
    "        print(\"Reached Leaf Node\");\n",
    "        print();\n",
    "        print();\n",
    "    else:\n",
    "        for i in range(len(temp)-1,-1,-1):\n",
    "            print(\"Count of\",temp[i][0],\"=\",temp[i][1]);\n",
    "        print(\"Current Entropy is =\",find_entropy(root.y));\n",
    "        print(\"Splitting on feature\",feature[root.feature],\"with gain ratio =\",root.gain_ratio);\n",
    "        print();\n",
    "        print();\n",
    "    if(root.left!=None):\n",
    "        print_tree(root.right,feature);\n",
    "        print_tree(root.left,feature);\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a function which predicts the output for a given input feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_i(i,node):\n",
    "    if(node.left==None and node.right==None):\n",
    "        return node.output;\n",
    "    elif(i[node.feature]<node.value):\n",
    "        return predict_for_i(i,node.right);\n",
    "    else:\n",
    "        return predict_for_i(i,node.left);\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating And Printing The Tree for the OR(Gate)...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of 0 = 1\n",
      "Count of 1 = 3\n",
      "Current Entropy is = 0.8112781244591328\n",
      "Splitting on feature x1 with gain ratio = 0.31127812445913283\n",
      "\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 1\n",
      "Count of 1 = 1\n",
      "Current Entropy is = 1.0\n",
      "Splitting on feature x2 with gain ratio = 1.0\n",
      "\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 1\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "\n",
      "Level 2\n",
      "Count of 1 = 1\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "\n",
      "Level 1\n",
      "Count of 1 = 2\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets;\n",
    "import math;\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np;\n",
    "from collections import Counter\n",
    "iris = datasets.load_iris()\n",
    "x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state = 1);\n",
    "#root=construct_tree(x_train,y_train,0);\n",
    "root=construct_tree(np.array([[1,1],[0,1],[1,0],[0,0]]),np.array([1,1,1,0]));\n",
    "print_tree(root,[\"x1\",\"x2\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Cell Is Used To Make and Print Tree For The Iris Dataset....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of 1 = 34\n",
      "Count of 0 = 37\n",
      "Count of 2 = 41\n",
      "Current Entropy is = 1.5807197138422104\n",
      "Splitting on feature petal length (cm) with gain ratio = 1.0000000000000002\n",
      "\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 37\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "\n",
      "Level 1\n",
      "Count of 1 = 34\n",
      "Count of 2 = 41\n",
      "Current Entropy is = 0.993707106604508\n",
      "Splitting on feature petal width (cm) with gain ratio = 0.6610420198933152\n",
      "\n",
      "\n",
      "Level 2\n",
      "Count of 2 = 4\n",
      "Count of 1 = 33\n",
      "Current Entropy is = 0.49418293484978865\n",
      "Splitting on feature petal length (cm) with gain ratio = 0.6941833044972409\n",
      "\n",
      "\n",
      "Level 3\n",
      "Count of 1 = 32\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "\n",
      "Level 3\n",
      "Count of 1 = 1\n",
      "Count of 2 = 4\n",
      "Current Entropy is = 0.7219280948873623\n",
      "Splitting on feature sepal length (cm) with gain ratio = 0.33155970728682876\n",
      "\n",
      "\n",
      "Level 4\n",
      "Count of 1 = 1\n",
      "Count of 2 = 1\n",
      "Current Entropy is = 1.0\n",
      "Splitting on feature sepal width (cm) with gain ratio = 1.0\n",
      "\n",
      "\n",
      "Level 5\n",
      "Count of 2 = 1\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "\n",
      "Level 5\n",
      "Count of 1 = 1\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "\n",
      "Level 4\n",
      "Count of 2 = 3\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "\n",
      "Level 2\n",
      "Count of 1 = 1\n",
      "Count of 2 = 37\n",
      "Current Entropy is = 0.17556502585750278\n",
      "Splitting on feature petal length (cm) with gain ratio = 0.18573556471891253\n",
      "\n",
      "\n",
      "Level 3\n",
      "Count of 1 = 1\n",
      "Count of 2 = 3\n",
      "Current Entropy is = 0.8112781244591328\n",
      "Splitting on feature sepal width (cm) with gain ratio = 1.0\n",
      "\n",
      "\n",
      "Level 4\n",
      "Count of 2 = 3\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "\n",
      "Level 4\n",
      "Count of 1 = 1\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "\n",
      "Level 3\n",
      "Count of 2 = 34\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state = 1)\n",
    "node1=construct_tree(x_train,y_train);\n",
    "print_tree(node1,iris.feature_names);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is just for printing the accuracy of our Decision Tree on Iris Dataset and some other error measuring techniques such as confusion matrix and classifier report..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction is-> 0.9736842105263158\n",
      "\n",
      "\n",
      "Confusion Matrix.....\n",
      "[[13  0  0]\n",
      " [ 0 15  1]\n",
      " [ 0  0  9]]\n",
      "\n",
      "\n",
      "Classifier report.....\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      0.94      0.97        16\n",
      "           2       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.97      0.98      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix;\n",
    "from sklearn.metrics import classification_report;\n",
    "y_pred=[];\n",
    "for i in range(len(x_test)):\n",
    "    y_pred.append(predict_for_i(x_test[i],node1));\n",
    "print(\"Accuracy of prediction is->\",accuracy_score(y_pred,y_test));\n",
    "print();\n",
    "print();\n",
    "print(\"Confusion Matrix.....\")\n",
    "print(confusion_matrix(y_pred=y_pred, y_true=y_test))\n",
    "print();\n",
    "print();\n",
    "print(\"Classifier report.....\")\n",
    "print(classification_report(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 0, 2, 1, 2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1, 2, 0, 2, 1, 0, 0, 1, 2, 1, 2, 1, 2, 2, 0, 1, 0]\n",
      "[0, 1, 1, 0, 2, 1, 2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 2, 1, 0, 0, 1, 2, 1, 2, 1, 2, 2, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred);\n",
    "print(list(y_test));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
